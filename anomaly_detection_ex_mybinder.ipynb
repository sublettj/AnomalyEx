{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f71bb0-e3de-4447-8317-fe5ceea0e0a4",
   "metadata": {},
   "source": [
    "# Example of Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51eb7c-daa8-4da1-ab7b-148473d23139",
   "metadata": {},
   "source": [
    "Written by Seulki Kim (As of August 14, 2021)\n",
    "\n",
    "Example is gotten from [Anomaly Detection in Time Series Sensor Data](https://towardsdatascience.com/anomaly-detection-in-time-series-sensor-data-86fd52e62538), which focuses on the application of anomaly detection in the manufacturing industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c1c42b-d297-4540-ac9f-b5b23e2f105e",
   "metadata": {},
   "source": [
    "## Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25acd0c1-48a7-44de-8927-eaf009feac7b",
   "metadata": {},
   "source": [
    "Manufacturing industry is considered a heavy industry in which they tend to utilize various types of heavy machinery such as giant motors, pumps, pipes, furnace,s conveyor belts, haul trucks, etc. These are often considered as the most critical assets for their operations. The integerity and reliability of these equipment is often the core focus.\n",
    "\n",
    "The failure of these equipment often results in production loss that could consequently lead to loss of hunderes of thousands of dolloars. Therefore, the ability to detect anomalies in advance and be able to mitigate risks is a very valuable capacity that further allows preventing an unplanned downtime and unncessary maintenance.\n",
    "\n",
    "In this post, different anomaly detection techiniques will be implemented in Python with `Scikit-learn` (a.k.a. sklearn) and our goal is going to be to search for anomalies in the time-series sensor readings from a pump with **unsupervised learning algorithms**. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e44e25-cdc7-4531-a468-9f8bf7bf8f64",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f046d9-9b0d-4404-aa70-fbee02edae96",
   "metadata": {},
   "source": [
    "It is very hard to find a public data from a manufacturing industry for this particular use case, but the author was able to find one that is not perfect. The dataset contains sensor reading from 53 sensors installed on a pump to measure various behaviors of the pump. This dataset can be found [here](https://www.kaggle.com/nphantawee/pump-sensor-data).\n",
    "\n",
    "Once downloaded, read the CSV file into the `pandas DataFrame` with the following code and check out the details of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b119c43d-a1e8-4aa9-8ed1-08e7e5b94bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\18503\\Downloads\n"
     ]
    }
   ],
   "source": [
    "# Import libraries necessary for this notebook\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c612b70-0824-4325-af6e-c1b329f5df15",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\calla\\\\OneDrive\\\\anomaly_detection\\\\sensor.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d451bbc883cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\calla\\OneDrive\\anomaly_detection\\sensor.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"encoding_errors\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         )\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    707\u001b[0m             )\n\u001b[0;32m    708\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\calla\\\\OneDrive\\\\anomaly_detection\\\\sensor.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('sensor.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2557ba33-0217-413f-81b4-48e03e7b5f34",
   "metadata": {},
   "source": [
    "We can already see that the data requires some cleaning, such as missing values, an empty column, and a timestamp with an incorrect data type. The we will apply the following steps to tidy up the dataset.\n",
    "* Remove redundant columns\n",
    "* Remove duplicates\n",
    "* Handle missing values\n",
    "* Convert data types to the correct data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b256bcc-acca-4076-8df4-00f829db8acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows, keeping first occurrence\n",
    "df = df.drop_duplicates()\n",
    "# Entire \"sensor_15\" column is NaN, so remove it from data\n",
    "del df['sensor_15']\n",
    "\n",
    "# Convert the data type of timestamp column to datetime format (will error if column has datetime values)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df['date'] = pd.to_datetime(df['timestamp'])\n",
    "del df['timestamp']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929aae75-367e-4050-bb43-905fc271b0c4",
   "metadata": {},
   "source": [
    "Next, let us handle the missing values and see the columns that have missing values and see what percentage of the data is missing. To do that, we writes the function that calculates the percentage of missing values so that the we can use the same function multiple times througout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b54e0-2beb-482c-bda5-e77569f11cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that calculates the percentage of missing values\n",
    "def calc_percent_NaN(df):\n",
    "    nans = pd.DataFrame(df.isnull().sum().sort_values(ascending=False)/len(df)*100, columns = ['percent'])\n",
    "    idx = nans['percent'] > 0\n",
    "    return nans[idx]\n",
    "\n",
    "# Use above function to look at top ten columns with NaNs\n",
    "calc_percent_NaN(df).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f57de7-4a55-40f4-a151-b90fed9f5565",
   "metadata": {},
   "source": [
    "After some analysis, the we decided to impute some of the missing values with their mean and drop the rest. After data wrangling process, the final tidy data looks as follows and is ready for the next step that is Exploratory Data Analysis. The tidy dataset has 52 sensors, machine status column that contains three classes (NORMAL, BROKEN, RECOVERING) that represents normal operating, broken, and recovering conditions of the pump, respectivley, and then the datetime column that represents the timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba10270d-bdba-4bab-9e12-cb60f47d9139",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6621fc4-a5db-4d42-b30e-24d438d71dab",
   "metadata": {},
   "source": [
    "Now that we have cleaned our data, we can start exploring to acquaint with the dataset.\n",
    "\n",
    "On top of some quantitative EDA, we perform additional graphical EDA to look for trends and any odd behaviors. In particular, it is interesting to see the sensor readings plotted over time with the machine status of \"BROKEN\" marked up on the same graph in red color. That way, we can clearly see when the pump breaks down and how that reflects in the sensor readings. The following code plots the mentioned graph for each sensors, but let us take a look at that for the sensor_00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7087cd6-d6bc-4e49-bf41-f35b4ae629e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace original index to 'date' column\n",
    "df.set_index('date', inplace=True, drop=True)\n",
    "\n",
    "# Extract the readings from the BROKEN sate of the pump\n",
    "broken = df[df['machine_status']=='BROKEN']\n",
    "\n",
    "# Remove the columns of 'Unnamed: 0' and 'machine_status'\n",
    "df2 = df.drop(['machine_status'], axis=1)\n",
    "df2 = df.drop(['Unnamed: 0'], axis=1)\n",
    "names=df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc31d057-0be5-4b5d-a9bc-cc4c842e8dee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot time series for each sensor with BROKEN state marked with X in red color\n",
    "for name in names:\n",
    "    _ = plt.figure(figsize=(18,3))\n",
    "    _ = plt.plot(broken[name], linestyle='none', marker='X', color='red', markersize=8)\n",
    "    _ = plt.plot(df[name], color='blue')\n",
    "    _ = plt.title(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e4eba5-2b1b-452e-99ff-dfd5b5b2eec5",
   "metadata": {},
   "source": [
    "As seen clearly from the above plot, the red marks, which represent the broken state of the pump, perfectly overlaps with the observed disturbances of the sensor reading. Now we have a pretty good intuition about how each of the snesor reading behaves when the pump is broken vs. operating normally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b53f89c-151e-4b77-9bbc-5bef780d2ef4",
   "metadata": {},
   "source": [
    "## Stationarity and Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b68ed-5d64-42e8-ac5b-97b4861042fe",
   "metadata": {},
   "source": [
    "**In time series analysis, it is important that the data is stationary and have no autocorrelation.** Stationarity refers to the behavior where the mean and standard deviation of the data changes over time, the data with such behavior is considered not stationary. On the other hand, autocorrelation refers to the behavior of the data where the data is correlated with itself in a different time period. As the next step, we will visually inspect the stationarity of each feature in the dataset and the following code will do that. Later, we will also perform the Dickey Fuller test to quantitatively verity the observed stationarity. In addtion, we will inspect the autocorrelation of the features before fedding them into the clustering algorithms to detect anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b607a14d-c088-4419-bb46-4548ec4b42ee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resample the entire dataset by daily average\n",
    "rollmean = df.resample(rule='D').mean()\n",
    "rollstd = df.resample(rule='D').std()\n",
    "\n",
    "# Plot time-series for each sensor with its mean and standard deviation\n",
    "for name in names:\n",
    "    _ = plt.figure(figsize=(18,3))\n",
    "    _ = plt.plot(df[name], color='blue', label='Original')\n",
    "    _ = plt.plot(rollmean[name], color='red', label='Rolling Mean')\n",
    "    _ = plt.plot(rollstd[name], color='black', label='Rolling Std')\n",
    "    _ = plt.legend(loc='best')\n",
    "    _ = plt.title(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de08f042-590a-4295-9db6-85889ae6ff5f",
   "metadata": {},
   "source": [
    "Looking at the readings from one of the sensors, 'sensor_17' in this case, notice that the data actually looks pretty stationary where the rolling mean and standard deviation don't seem to change over time except during the downtime of the pump that is expected. This was the case for most of the sensors in this dataset but it may not always be the case in which situations various transformation methods must be applied to make the data stationary before training the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868bbc52-4917-414c-84c8-e8e5945b3d92",
   "metadata": {},
   "source": [
    "## Pre-Processing and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2f619b-e447-4a34-9116-780cf0853bd4",
   "metadata": {},
   "source": [
    "It is pretty computationally expensive to train models with all of the 51 sensors/features and it is not efficient. Therefore, we will employ Principal Component Analysis (PCA) technique to extract new features to be used for the modeling. To properly apply PCA, the data must be scaled and standardized because PCA and most of the learning algorithms are distance-based algorithms. If noticed from the first 10 rows of the dity data, the magnitude of the values from each feature is not consistent. We will perform the following steps using the Pipeline library.\n",
    "1. Scale the data\n",
    "2. Perform PCA and look at the most important principal components based on inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70081736-b171-4b77-b2f2-a9dc0f88c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize/scale the dataset and apply PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Extract the names of the numerical columns\n",
    "df2 = df.drop(['machine_status'], axis=1)\n",
    "# Replace all NaNs with zeros\n",
    "df2 = df2.replace(np.nan, 0, regex=True)\n",
    "names = df2.columns\n",
    "x = df2[names]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "pipeline = make_pipeline(scaler, pca)\n",
    "pipeline.fit(x)\n",
    "\n",
    "# Plot the principal components against their inertia\n",
    "features = range(pca.n_components_)\n",
    "_ = plt.figure(figsize=(15, 5))\n",
    "_ = plt.bar(features, pca.explained_variance_)\n",
    "_ = plt.xlabel('PCA Feature')\n",
    "_ = plt.ylabel('Variance')\n",
    "_ = plt.xticks(features)\n",
    "_ = plt.title(\"Importance of the Principal Components based on inertia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7c9304-0495-4dd7-b1ca-f41b3269ed6a",
   "metadata": {},
   "source": [
    "It appears that the first two principal components are the most important as per the features extracted by the PCA in above importance plot. As the next step, we will perform PCA with two components that will be my features to be used in the training of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2e4c3-586b-4cde-8299-b9bf0550610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PCA with two components\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns=['pc1', 'pc2'])\n",
    "print(principalDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c41a1-8269-47c2-a90d-fe89e8d7909f",
   "metadata": {},
   "source": [
    "Now, we will check again the stationarity and autocorrelation of these two principal components to be sure that they are stationary and not autocorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3440645-0a9a-409d-a78f-67df386d2641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Run Augmented Dickey Fuller Test\n",
    "result = adfuller(principalDf['pc2'])\n",
    "\n",
    "# Print p-value\n",
    "print(result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e29bbac-2b16-46ef-9d9a-3f2d58dd633f",
   "metadata": {},
   "source": [
    "Running the Dickey Fuller test on the 1st principal component, we got a p-value of 0.9634 that is very small number *(?.. not sure why different number comes out compared to reference website)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b09421-f41a-492f-acd1-d8a6ea5b0eaf",
   "metadata": {},
   "source": [
    "Now, let us check for autocorrelation in both of these prinicipal components. It can be done one of the two ways; either with the pandas `autocorr()` method or `ACF plot`. We will use the latter in this case to quickly visually verify that there is no autocorrelation. The following code does that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d8569-7843-4155-a68e-81beefc68d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ACF\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(pc1.dropna(), lags=20, alpha=0.05) # something in code is missing in reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3234fd8-70f9-4e59-99a4-9886005e4acf",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f578f7-1222-468e-bd44-8590dc89f0b1",
   "metadata": {},
   "source": [
    "In this step, we will perform the following learning algorithms to detect anomalies.\n",
    "1. Benchmark model: Interquartile Range (IQR)\n",
    "2. K-Means clustring\n",
    "Let us start training with these algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cd90d0-dc54-4d8f-963b-1a2749f6b7df",
   "metadata": {},
   "source": [
    "### 1. Interquartile Range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf26cbc-92f7-4375-abf8-520d15ce9817",
   "metadata": {},
   "source": [
    "Strategy:\n",
    "1. Calculate IQR that is the difference between 75th (Q3) AND 25th (Q1) percentiles.\n",
    "2. Calculate upper and lower bounds for the outlier.\n",
    "3. Filter the data points that fall outside the upper and lower bounds and flag them as outliers.\n",
    "4. Finally, plot the outliers on top of the time-series data (the readings from 'sensor_11' in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6354e0c-ed04-4add-938c-595d182900d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for pc1\n",
    "# Calculate IQR for the 1st principal component (pc1)\n",
    "q1_pc1, q3_pc1 = principalDf['pc1'].quantile([0.25, 0.75])\n",
    "iqr_pc1 = q3_pc1 - q1_pc1\n",
    "\n",
    "# Calculate upper and lower bounds for outlier for pc1\n",
    "lower_pc1 = q1_pc1 - 1.5*iqr_pc1\n",
    "upper_pc1 = q3_pc1 + 1.5*iqr_pc1\n",
    "\n",
    "# Filter out the outliers from the pc1\n",
    "principalDf['anomaly_pc1'] = ((principalDf['pc1']>upper_pc1) | (principalDf['pc1']<lower_pc1)).astype('int')\n",
    "\n",
    "## for pc2\n",
    "# Calculate IQR for the 2nd principal component (pc2)\n",
    "q1_pc2, q3_pc2 = principalDf['pc2'].quantile([0.25, 0.75])\n",
    "iqr_pc2 = q3_pc2 - q1_pc2\n",
    "\n",
    "# Calculate upper and lower bounds for outlier for pc1\n",
    "lower_pc2 = q1_pc2 - 1.5*iqr_pc2\n",
    "upper_pc2 = q3_pc2 + 1.5*iqr_pc2\n",
    "\n",
    "# Filter out the outliers from the pc2\n",
    "principalDf['anomaly_pc2'] = ((principalDf['pc2']>upper_pc2) | (principalDf['pc2']<lower_pc2)).astype('int')\n",
    "\n",
    "# Plot the outliers from pc1 on top of the 'sensor_11' and see where they occured in the time-series\n",
    "df['anomaly_pc1'] = pd.Series(principalDf['anomaly_pc1'].values, index=df.index)\n",
    "a = df.loc[df['anomaly_pc1'] == 1] # anomaly\n",
    "_ = plt.figure(figsize=(18,6))\n",
    "_ = plt.plot(df['sensor_11'], color='blue', label='Normal')\n",
    "_ = plt.plot(a['sensor_11'], linestyle='none', marker='X', color='red', markersize=8, label='Anomaly')\n",
    "_ = plt.xlabel('Date and Time')\n",
    "_ = plt.ylabel('Sensor Reading')\n",
    "_ = plt.title('Sensor_11 Anomalies')\n",
    "_ = plt.legend(loc='best')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8271b6-d856-4583-bdd3-63bc042c7a4c",
   "metadata": {},
   "source": [
    "### 2. K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f757121e-4f07-4fff-bb74-ebf2dd9ee547",
   "metadata": {},
   "source": [
    "Strategy:\n",
    "1. Calculate the distance between each point and its nearest centroid. The biggest distances are considered as anomaly.\n",
    "2. We use `outliers_fraction` to provide information to the algorithm about proportion of the outliers present in our datset. Situations may vary from dataset to dataset. However, as a starting figure, the author estimate outliers_fraction=0.13 (13% of df are outliers as depicted).\n",
    "3. Calculate `number_of_outliers` using `outliers_fraction`.\n",
    "4. Set threshold as the minimum distance of these outliers.\n",
    "5. The anomaly result of `anomaly1` constains the above method Cluster (0: normal, 1: anomaly).\n",
    "6. Visualize anomalies with Time Series view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8763d-eaf7-44e1-98a7-9e5bcd1bda18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Will start k-means clustering with k=2 as we already know that there are 3 classes of 'NORMAL' vs. \n",
    "# \"NOT NORMAL\" which are combination of 'BROKEN' and 'RECOVERING'\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(principalDf.values)\n",
    "labels = kmeans.predict(principalDf.values)\n",
    "unique_elements, counts_elements = np.unique(labels, return_counts=True)\n",
    "clusters = np.asarray((unique_elements, counts_elements))\n",
    "\n",
    "# Write a function that calculates distance between each point and the centroid of the closest cluster\n",
    "def getDistanceByPoint(data, model):\n",
    "    \"\"\" Function that calculates the distance between a point and centroid of a cluster, \n",
    "            returns the distances in pandas series\"\"\"\n",
    "    distance = []\n",
    "    for i in range(0,len(data)):\n",
    "        Xa = np.array(data.loc[i])\n",
    "        Xb = model.cluster_centers_[model.labels_[i]-1]\n",
    "        distance.append(np.linalg.norm(Xa-Xb))\n",
    "    return pd.Series(distance, index=data.index)\n",
    "\n",
    "# Assume that 13% of the entire data set are anomalies \n",
    "outliers_fraction = 0.13\n",
    "\n",
    "# get the distance between each point and its nearest centroid. The biggest distances are considered as anomaly\n",
    "distance = getDistanceByPoint(principalDf, kmeans)\n",
    "\n",
    "# number of observations that equate to the 13% of the entire data set\n",
    "number_of_outliers = int(outliers_fraction*len(distance))\n",
    "\n",
    "# Take the minimum of the largest 13% of the distances as the threshold\n",
    "threshold = distance.nlargest(number_of_outliers).min()\n",
    "\n",
    "# anomaly1 contain the anomaly result of the above method Cluster (0:normal, 1:anomaly) \n",
    "principalDf['anomaly1'] = (distance >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692495a-d15e-406a-b8f0-99bc6d12791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the outliers from anomaly1 on top of the 'sensor_11' and see where they occured in the time-series\n",
    "df['anomaly1'] = pd.Series(principalDf['anomaly1'].values, index=df.index)\n",
    "b = df.loc[df['anomaly1'] == 1] # anomaly\n",
    "_ = plt.figure(figsize=(18,6))\n",
    "_ = plt.plot(df['sensor_11'], color='blue', label='Normal')\n",
    "_ = plt.plot(b['sensor_11'], linestyle='none', marker='X', color='red', markersize=8, label='Anomaly')\n",
    "_ = plt.xlabel('Date and Time')\n",
    "_ = plt.ylabel('Sensor Reading')\n",
    "_ = plt.title('Sensor_11 Anomalies')\n",
    "_ = plt.legend(loc='best')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bab288-fa89-4a0a-9b78-653b1ba80764",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b38f77-113c-444d-8ca1-d4a2ec5b868e",
   "metadata": {},
   "source": [
    "We have done anomaly detection with two different methods. In doing so, we went through most of the steps of the commonly applied Data Science Process which incldues the following steps:\n",
    "1. Problem identification\n",
    "2. Data wrangling\n",
    "3. Exploratory data analysis\n",
    "4. Pre-processing and training data development\n",
    "5. Modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
